function [CCF,forestPredictsTest,forestProbsTest,treeOutputTest] = ...
    genCCF(nTrees,XTrain,YTrain,bReg,optionsFor,XTest,bKeepTrees,iFeatureNum,bOrdinal)
%genCCF Generate a canonical correlation forest
%
% Typical usage:
%   CCF = genCCF(nTrees,XTrain,YTrain,bReg)
%
% Creates a canonical correlation forest (CCF) comprising of nTrees
% canonical correlation trees (CCT) containing splits based on the a CCA
% analysis between the training data and a binary representation of the
% class labels.
%
% Required Inputs:
%         nTrees = Number of trees to create (default 500)
%         XTrain = Array giving training features.  Each row should be a
%                  seperate data point and each column a seperate feature.
%                  Must be numerical array with missing values marked as
%                  NaN if iFeatureNum is provided, otherwise can be any
%                  format accepted by processInputData function
%         YTrain = Output data. Accepted formats as follows (note that
%                  loadCSVDataSet and loadProvidedDataset automatically
%                  give correct format).
%                  Regression:
%                       Column vector of outputs
%                  Multivariate regression:
%                       Matrix where each column is a different output
%                  Classification:
%                       There three excepted formats: a column vector of
%                       integers representing class labels, a cell column
%                       vector of strings giving the class name, or a NxK
%                       logical array representing a 1-of-K encoding of the
%                       class labels.  For binary classification a logical
%                       column vector is also accepted.
%                   Multi-output classification:
%                       Two accepted formats: a NxNout array of numeric
%                       class labels where each column is a different
%                       output or a 1xNout cell array where each cell is a
%                       seperate input satisfying the requirements for a
%                       single classification.
%           bReg = Whether to perform regression instead of classification.
%                  Default = false (i.e. classification).
%
% Advanced usage:
%
% [CCF, forPred, forProbs, treeOutputs] =
%  genCCF(nTrees,XTrain,YTrain,bReg,options,XTest,bKeepTrees,iFeatureNum,bOrdinal)
%
% Options Inputs:
%        options = Options object created by optionsClassCCF.  If left
%                  blank then a default set of options corresponding to the
%                  method detailed in the paper is used.
%          XTest = Test data to make predictions for.  If the input
%                  features for the test data are known at test time then
%                  using this input with the option bKeepTrees = false can
%                  significantly reduce the memory requirement.
%     bKeepTrees = If false and XTest is given then the individual trees
%                  are not stored in order to save memory.  Default = true
%    iFeatureNum = Vector for grouping of categorical variables as
%                  generated by processInputData function.  If left blank
%                  then the data is processed using processInputData.
%       bOrdinal = If the data is to be processed, this allows
%                  specification of ordinal variables.  For default
%                  behaviour see processInputData.m
%
% Outputs:
%            CCF = Structure with following fields
%                    - Trees = Cell array of CCTs
%                    - bReg = Whether a regression CCF
%                    - options = The used options structure (after
%                       processing for things like task_ids based on the
%                       data)
%                    - inputProcessDetails = Details required to replicate
%                       the input feature transforms (e.g. converting to
%                       z-scores) done during training
%                    - outOfBagError = if bagging was used, gives the
%                       average out of bag error.  Otherwise empty.
%                    - timing_stats = see bCalcTimingStats option in
%                       optionsClassCCF.m
%        forPred = Forest predictions for XTest if provided
%       forProbs = Forest probabilities for XTest if provided
%    treeOutputs = Individual tree predictiosn for XTest if provided, see
%                  predictFromCCF
%
% Tom Rainforth 23/07/17

% Set ommited options
if ~exist('nTrees','var') || isempty(nTrees)
    nTrees = 500;
end

if ~exist('bReg','var') || isempty(bReg)
    bReg = false;
end

if ~exist('bOrdinal','var')
    bOrdinal = [];
elseif ~isempty(bOrdinal)
    bOrdinal = logical(bOrdinal);
end

if ~exist('optionsFor','var') || isempty(optionsFor)
    if bReg
        optionsFor = optionsClassCCF.defaultOptionsReg;
    else
        optionsFor = optionsClassCCF;
    end
end

tStartAll = tic;

bNaNtoMean = strcmpi(optionsFor.missingValuesMethod,'mean');

if ~isnumeric(XTrain) || ~exist('iFeatureNum','var') || isempty(iFeatureNum)
    % If XTrain not in numeric form or if a grouping of features is not
    % provided, apply the input data processing.
    if exist('iFeatureNum','var') && ~isempty(iFeatureNum)
        warning('iFeatureNum provided but XTrain not in array format, over-riding');
    end
    if ~exist('XTest','var') || isempty(XTest)
        [XTrain, iFeatureNum, inputProcessDetails] = processInputData(XTrain,bOrdinal,[],bNaNtoMean);
    else
        [XTrain, iFeatureNum, inputProcessDetails, XTest] = processInputData(XTrain,bOrdinal,XTest,bNaNtoMean);
    end
else
    % Process inputs, e.g. converting categoricals and converting to
    % z-scores
    mu_XTrain = nanmean(XTrain,1);
    std_XTrain = nanstd(XTrain,[],1);
    inputProcessDetails = struct('bOrdinal',true(1,size(XTrain,2)),'mu_XTrain',mu_XTrain,'std_XTrain',std_XTrain);
    inputProcessDetails.Cats = cell(0,1);
    XTrain = replicateInputProcess(XTrain,inputProcessDetails);
    if ~isempty(XTest)
        XTest = replicateInputProcess(XTest,inputProcessDetails);
    end
end

if ~exist('bKeepTrees','var') || isempty(bKeepTrees)
    bKeepTrees = true;
end

N = size(XTrain,1);
D = numel(fastUnique(iFeatureNum)); % Note that setting of number of features to subsample is based only
% number of features before expansion of categoricals.


if ~bReg
    % Process provided classes
    
    [YTrain, classes, optionsFor] = classExpansion(YTrain,N,optionsFor);
    
    if numel(classes)==1
        warning('Only 1 class present in training data!');
    end
    
    optionsFor = optionsFor.updateForD(D);
    
    % Stored class names can be used to link the ids given in the CCT to the
    % actual class names
    optionsFor.classNames = classes;
    
else
    % Center and normalize the outputs for regression for numerical
    % reasons, this is undone in the predictors
    
    muY = mean(YTrain);
    stdY = std(YTrain,[],1);
    
    % TODO do something more efficient here
    % For now just set stdY to be 1 instead of zero to prevent NaNs if a
    % dimensions has no variation.
    stdY(stdY==0) = 1;
    
    YTrain = bsxfun(@rdivide,bsxfun(@minus,YTrain,muY),stdY);
    
    optionsFor = optionsFor.updateForD(D);
    optionsFor.org_muY = muY;
    optionsFor.org_stdY = stdY;
    optionsFor.mseTotal = 1;
end

% Fill in any unset projection fields and set to false
projection_fields = {'CCA','PCA','CCAclasswise','Original','Random'};
for npf = 1:numel(projection_fields)
    if ~isfield(optionsFor.projections,projection_fields{npf})
        optionsFor.projections.(projection_fields{npf}) = false;
    end
end
% Ensure consistent order
optionsFor.projections = orderfields(optionsFor.projections,projection_fields);

nOut = nargout;

if nOut<2 && ~bKeepTrees
    bKeepTrees = true;
    warning('Selected not to keep trees but only requested a single output of the trees, reseting bKeepTrees to true');
end

if ~exist('XTest','var') || isempty(XTest)
    if nOut>1
        error('To return more than just the trees themselves, must input the test points');
    else
        XTest = NaN(0,size(XTrain,1));
    end
end

forest = cell(1,nTrees);
if nOut>1
    treeOutputTest = NaN(size(XTest,1),nTrees,size(YTrain,2));
end

n_nodes_trees = NaN(nTrees,1);
tree_train_times = NaN(nTrees,1);
if nOut>1
    tree_test_times = NaN(nTrees,1);
end

% Train the trees
if optionsFor.bUseParallel == true
    parfor nT = 1:nTrees
        tStartTrainThis = tic;
        tree = genTree(XTrain,YTrain,bReg,optionsFor,iFeatureNum,N);
        if optionsFor.bCalcTimingStats
            % Avoid some calculations if not doing the timing stats
            tree_train_times(nT) = toc(tStartTrainThis);
            n_nodes_trees(nT) = get_number_of_nodes(tree);
        end
        if bKeepTrees
            forest{nT} = tree;
        end
        if nOut>1
            tStartTestThis = tic;
            treeOutputTest(:,nT,:) = predictFromCCT(tree,XTest);
            tree_test_times(nT) = toc(tStartTestThis);
        end
    end
else
    for nT = 1:nTrees
        tStartTrainThis = tic;
        tree = genTree(XTrain,YTrain,bReg,optionsFor,iFeatureNum,N);
        if optionsFor.bCalcTimingStats
            % Avoid some calculations if not doing the timing stats
            tree_train_times(nT) = toc(tStartTrainThis);
            n_nodes_trees(nT) = get_number_of_nodes(tree);
        end
        if bKeepTrees
            forest{nT} = tree;
        end
        if nOut>1
            tStartTestThis = tic;
            treeOutputTest(:,nT,:) = predictFromCCT(tree,XTest);
            tree_test_times(nT) = toc(tStartTestThis);
        end
    end
end

% Setup outputs
CCF.Trees = forest;
CCF.bReg = bReg;
CCF.options = optionsFor;
CCF.inputProcessDetails = inputProcessDetails;
CCF.classNames = optionsFor.classNames;

if optionsFor.bBagTrees && bKeepTrees
    % Calculate the out of back error if relevant
    cumOOb = zeros(size(YTrain,1),size(CCF.Trees{1}.predictsOutOfBag,2));
    nOOb = zeros(size(YTrain,1),1);
    for nTO = 1:numel(CCF.Trees)
        cumOOb(CCF.Trees{nTO}.iOutOfBag,:) = cumOOb(CCF.Trees{nTO}.iOutOfBag,:)+CCF.Trees{nTO}.predictsOutOfBag;
        nOOb(CCF.Trees{nTO}.iOutOfBag) = nOOb(CCF.Trees{nTO}.iOutOfBag)+1;
    end
    oobPreds = bsxfun(@rdivide,cumOOb,nOOb);
    if bReg
        CCF.outOfBagError = nanmean((oobPreds-bsxfun(@plus,bsxfun(@times,YTrain,stdY),muY)).^2,1);
    elseif optionsFor.bSepPred
        CCF.outOfBagError = (1-nanmean((oobPreds>0.5)==YTrain,1));
    else
        forPreds = NaN(size(XTrain,1),numel(optionsFor.task_ids));
        YTrainCollapsed = NaN(size(XTrain,1),numel(optionsFor.task_ids));
        for nO = 1:(numel(optionsFor.task_ids)-1)
            [~,forPreds(:,nO)] = max(oobPreds(:,optionsFor.task_ids(nO):optionsFor.task_ids(nO+1)-1),[],2);
            [~,YTrainCollapsed(:,nO)] = max(YTrain(:,optionsFor.task_ids(nO):optionsFor.task_ids(nO+1)-1),[],2);
        end
        [~,forPreds(:,end)] = max(oobPreds(:,optionsFor.task_ids(end):end),[],2);
        [~,YTrainCollapsed(:,end)] = max(YTrain(:,optionsFor.task_ids(end):end),[],2);
        CCF.outOfBagError = (1-nanmean(forPreds==YTrainCollapsed,1));
    end
else
    CCF.outOfBagError = 'OOB error only returned if bagging used and trees kept.  Please use CCF-Bag instead via options=optionsClassCCF.defaultOptionsCCFBag';
end

if nOut>1
    % Do predictions if requested
    [forestPredictsTest, forestProbsTest] = treeOutputsToForestPredicts(CCF,treeOutputTest);
end

if optionsFor.bCalcTimingStats
    total_time = toc(tStartAll);
    CCF.timing_stats.total_wall_time = total_time;
    CCF.timing_stats.cpu_train_time = sum(tree_train_times);
    CCF.timing_stats.cpu_tree_trains = tree_train_times;
    if nOut>1
        CCF.timing_stats.cpu_test_time = sum(tree_test_times);
        CCF.timing_stats.cpu_tree_test_times = tree_test_times;
    end
    CCF.timing_stats.n_nodes_trees = n_nodes_trees;
end

end

function tree = genTree(XTrain,YTrain,bReg,optionsFor,iFeatureNum,N)
% A sub-function is used so that it can be shared between the for and
% parfor loops.  Does required preprocessing such as randomly setting
% missing values, then calls the tree training function

if strcmpi(optionsFor.missingValuesMethod,'random')
    % Randomly set the missing values.  This will be different for each
    % tree
    XTrain = random_missing_vals(XTrain);
end

% Bag if required
if optionsFor.bBagTrees
    iTrainThis = datasample(1:N,N);
    iOob = setdiff(1:N,iTrainThis)';
    XTrainOrig = XTrain;
    XTrain = XTrain(iTrainThis,:);
    YTrain = YTrain(iTrainThis,:);
end

% Apply pre rotations if any requested.  Note that these all include a
% subtracting a the mean prior to the projection (because this is a natural
% part of pca) and this is therefore replicated at test time
if strcmpi(optionsFor.treeRotation,'rotationForest')
    % This allows functionality to use the Rotation Forest algorithm as a
    % meta method for individual CCTs
    prop_classes_eliminate = optionsFor.RotForpClassLeaveOut;
    if bReg
        prop_classes_eliminate = 0;
    end
    [R,muX,XTrain] = rotationForestDataProcess(XTrain,YTrain,optionsFor.RotForM,...
        optionsFor.RotForpS,prop_classes_eliminate);
elseif strcmpi(optionsFor.treeRotation,'random')
    muX = nanmean(XTrain,1);
    R = randomRotation(size(XTrain,2));
    XTrain = bsxfun(@minus,XTrain,muX)*R;
elseif strcmpi(optionsFor.treeRotation,'pca')
    [R,muX,XTrain] = pcaLite(XTrain,false,false);
end

% Train the tree
tree = growCCT(XTrain,YTrain,bReg,optionsFor,iFeatureNum,0);

% Calculate out of bag error if relevant
if optionsFor.bBagTrees
    tree.iOutOfBag = iOob;
    tree.predictsOutOfBag = predictFromCCT(tree,XTrainOrig(iOob,:));
end

% Store rotation deatils if necessary
if ~strcmpi(optionsFor.treeRotation,'none')
    tree.rotDetails = struct('R',R,'muX',muX);
end

end
